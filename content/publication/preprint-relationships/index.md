---

title: "The Code That Binds Us: Navigating the Appropriateness of Human-AI Assistant Relationships"
authors: 
- Arianna Manzini
- Geoff Keeling
- admin 
- Shannon Vallor
- Meredith Ringel Morris
- Iason Gabriel

date: "2024"
# doi: "https://doi.org/10.48550/arXiv.2302.04720"

# Schedule page publish date (NOT publication's date).
publishDate: "2024-05-16"

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ["article"]

# Publication name and optional abbreviated publication name.
publication: "*Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)*, 7(1), 943-957. https://doi.org/10.1609/aies.v7i1.31694"
publication_short: "*Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)*, 7(1), 943-957. https://doi.org/10.1609/aies.v7i1.31694"

# Publication name and optional abbreviated publication name.
# publication: "Computers as Bad Social Actors: Dark Patterns and Anti-Patterns in Interfaces that Act Socially"
# publication_short: "Computers as Bad Social Actors"
abstract: This paper explores the moral limits of relationships between users and advanced AI assistants, specifically which features of such relationships render them appropriate or inappropriate. We first consider a series of values including benefit, flourishing, autonomy and care that are characteristic of appropriate human interpersonal relationships. We use these values to guide an analysis of which features of user–AI assistant relationships are liable to give rise to harms, and then we discuss a series of risks and mitigations for such relationships. The risks that we explore are - (1) causing direct emotional and physical harm to users; (2) limiting opportunities for user personal development; (3) exploiting emotional dependence; and (4) generating material dependencies.



# Summary. An optional shortened abstract.

summary: This paper explores the moral limits of relationships between users and advanced AI assistants, specifically which features of such relationships render them appropriate or inappropriate. We first consider a series of values including benefit, flourishing, autonomy and care that are characteristic of appropriate human interpersonal relationships. We use these values to guide an analysis of which features of user–AI assistant relationships are liable to give rise to harms, and then we discuss a series of risks and mitigations for such relationships. The risks that we explore are - (1) causing direct emotional and physical harm to users; (2) limiting opportunities for user personal development; (3) exploiting emotional dependence; and (4) generating material dependencies.

tags:
- Advanced AI Assistants
- Conversational Agents
- Social Interactional Harms
- Human-AI relationships

featured: false

# links:
- name: DOI
  url: https://doi.org/10.1609/aies.v7i1.31694
 url_pdf: https://ojs.aaai.org/index.php/AIES/article/view/31694/33861


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ''
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
# projects:


# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
# slides: example
---


