---

title: "What Makes for a 'Good' Social Actor? Using Respect as a Lens to Evaluate Interactions with Language Agents"
authors: 
- Alberts, L
- Keeling, G.
- McCroskery, A.
date: "2024"
# doi: "https://doi.org/10.48550/arXiv.2302.04720"

# Schedule page publish date (NOT publication's date).
publishDate: "2024-01-17"

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ["article"]

# Publication name and optional abbreviated publication name.
publication: "Under review at *Minds and Machines*. arXiv:2401.09082 [cs.CL]"
publication_short: "Under review at *Minds and Machines*.	arXiv:2401.09082 [cs.CL]"

# Publication name and optional abbreviated publication name.
# publication: "Computers as Bad Social Actors: Dark Patterns and Anti-Patterns in Interfaces that Act Socially"
# publication_short: "Computers as Bad Social Actors"
abstract: With the growing popularity of  dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the ‘HHH’ criteria for making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and  offers practical suggestions to help LLM technologies behave as good social actors and treat people respectfully.



# Summary. An optional shortened abstract.

summary: Critiquing the harmless, honest, helpful ('HHH') framework for LLM alignment, we propose an interactional ethics that is more centred on pragmatic factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as good social actors and treat people respectfully. 

tags:
- Large Language Model Alignment 
- Dialogue Agents
- Social Interactional Harms
- Respect
- Interactional Ethics
- Self-Determination Theory
- The Ethics of Care

featured: false

links:
- name: DOI
  url: https://doi.org/10.48550/arXiv.2401.09082
url_pdf: https://arxiv.org/pdf/2401.09082.pdf


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ''
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
# projects:


# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
# slides: example
---


