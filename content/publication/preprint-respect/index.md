---

title: "Should agentic conversational AI change how we think about ethics? Characterising an interactional ethics centred on respect"
authors: 
- admin
- Geoff Keeling
- Amanda McCroskery
date: "2024"
# doi: "https://doi.org/10.48550/arXiv.2302.04720"

# Schedule page publish date (NOT publication's date).
publishDate: "2024-05-16"

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ["article"]

# Publication name and optional abbreviated publication name.
publication: "Under review at *AAAI Conference on AI, Ethics, and Society (AIES)*. arXiv:2401.09082v2 [cs.CL]"
publication_short: "Under review at *AAAI Conference on AI, Ethics, and Society (AIES)*.	arXiv:2401.09082v2 [cs.CL]"

# Publication name and optional abbreviated publication name.
# publication: "Computers as Bad Social Actors: Dark Patterns and Anti-Patterns in Interfaces that Act Socially"
# publication_short: "Computers as Bad Social Actors"
abstract: With the growing popularity of conversational agents based on large language models (LLMs), we need to ensure their behaviour is ethical and appropriate. Work in this area largely centres around the 'HHH’ criteria - making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus isu seful when viewing LLM agents as mere mediums or output-generating systems, it fails to account for pragmatic factors that can make the same speech act seem more or less tactless or inconsiderate in different social situations. With the push towards agentic AI, wherein systems become increasingly proactive in chasing goals and performing actions in the world, considering the pragmatics of interaction becomes essential. We propose an interactional approach to ethics that is centred on relational and situational factors. We explore what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated social interaction, and offers practical suggestions to help agentic LLM technologies treat people well.



# Summary. An optional shortened abstract.

summary: Until now, our understanding of what it means for generative AI like large language models (LLMs) to behave ethically has mainly considered semantics (e.g., ensuring outputs do not contain any biased, inaccurate, harmful, offensive or toxic language). However, as AI systems start behaving more like social actors—speaking directly to people in natural language, and becoming more proactive in doing so—we believe that the pragmatics of situated social interaction should get more attention. That is, more than thinking about what makes for helpful or harmful language in the abstract, we need to consider what it actually means to treat a person well in an interaction or ongoing relationship. More than just avoiding universal 'harms' like being sexist or misleading, we propose an interactional ethics that is centred on duties of respect, considering how situational, relational and individual factors can make the same speech act seem more or less rude or inconsiderate in different contexts. 

tags:
- Large Language Model Ethics and Evaluation
- Conversational Agents
- Social Interactional Harms
- Respect
- Interactional Ethics
- Self-Determination Theory
- The Ethics of Care

featured: false

links:
- name: DOI
  url: https://doi.org/10.48550/arXiv.2401.09082
url_pdf: https://arxiv.org/pdf/2401.09082


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ''
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
# projects:


# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
# slides: example
---


